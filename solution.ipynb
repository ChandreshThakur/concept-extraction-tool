{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "36a97965",
   "metadata": {},
   "source": [
    "# Concept Extraction from Competitive Exam Questions\n",
    "\n",
    "This notebook demonstrates the concept extraction system that combines RAKE (Rapid Automatic Keyword Extraction) with custom keyword dictionaries to identify concepts from competitive exam questions.\n",
    "\n",
    "## Project Features\n",
    "\n",
    "- **Hybrid Concept Extraction**: Uses RAKE + Custom Dictionary\n",
    "- **Cost-Effective**: No direct LLM API calls for core functionality\n",
    "- **Future-Proof**: Built with LLM integration interface\n",
    "- **Domain Adaptable**: Works across multiple subjects\n",
    "- **CSV-based I/O**: Standard file formats for input and output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b79b007",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7eba5380",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'nltk'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Download necessary NLTK data\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'nltk'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "import os\n",
    "\n",
    "# Download necessary NLTK data\n",
    "nltk_data_to_download = [\n",
    "    ('tokenizers/punkt', 'punkt'),\n",
    "    ('tokenizers/punkt_tab', 'punkt_tab'),\n",
    "    ('corpora/stopwords', 'stopwords')\n",
    "]\n",
    "\n",
    "for data_path, package_name in nltk_data_to_download:\n",
    "    try:\n",
    "        nltk.data.find(data_path)\n",
    "    except LookupError:\n",
    "        print(f\"Downloading {package_name}...\")\n",
    "        nltk.download(package_name)\n",
    "\n",
    "# Import our custom modules\n",
    "from csv_reader import read_questions_csv\n",
    "from concept_extractor import ConceptExtractor\n",
    "from simulated_llm import SimulatedLLM\n",
    "\n",
    "print(\"Setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbda49f6",
   "metadata": {},
   "source": [
    "## 2. Load Sample Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25a73c54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load ancient history questions\n",
    "questions_file = \"resources/ancient_history.csv\"\n",
    "questions_df = read_questions_csv(questions_file)\n",
    "\n",
    "print(f\"Loaded {len(questions_df)} questions\")\n",
    "print(\"\\nSample questions:\")\n",
    "print(questions_df[['Question Number', 'Question']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6da533fd",
   "metadata": {},
   "source": [
    "## 3. Hybrid Concept Extraction (RAKE + Custom Dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3289691c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize hybrid concept extractor\n",
    "custom_dict_file = \"dictionaries/ancient_history_concepts.csv\"\n",
    "hybrid_extractor = ConceptExtractor(custom_dict_file=custom_dict_file)\n",
    "\n",
    "# Extract concepts from all questions\n",
    "questions_with_concepts = hybrid_extractor.extract_concepts_from_dataframe(questions_df.copy())\n",
    "\n",
    "print(\"Concept extraction complete!\")\n",
    "print(\"\\nResults:\")\n",
    "for idx, row in questions_with_concepts.iterrows():\n",
    "    print(f\"\\nQuestion {row['Question Number']}: {row['Question'][:100]}...\")\n",
    "    print(f\"Concepts: {row['Concepts']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7955cbcc",
   "metadata": {},
   "source": [
    "## 4. View Custom Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c26e6924",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and display the custom dictionary\n",
    "custom_dict_df = pd.read_csv(custom_dict_file)\n",
    "print(\"Custom Dictionary for Ancient History:\")\n",
    "print(custom_dict_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4300edfb",
   "metadata": {},
   "source": [
    "## 5. Simulated LLM Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9daec28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test simulated LLM extraction\n",
    "simulated_llm = SimulatedLLM()\n",
    "\n",
    "print(\"Simulated LLM Concept Extraction:\")\n",
    "for idx, row in questions_df.iterrows():\n",
    "    concepts = simulated_llm.extract_concepts(row['Question'])\n",
    "    print(f\"\\nQuestion {row['Question Number']}: {row['Question'][:100]}...\")\n",
    "    print(f\"LLM Concepts: {'; '.join(concepts)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed1dd103",
   "metadata": {},
   "source": [
    "## 6. Comparison: Hybrid vs Simulated LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46bd3efb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comparison DataFrame\n",
    "comparison_df = questions_df[['Question Number', 'Question']].copy()\n",
    "comparison_df['Hybrid_Concepts'] = questions_with_concepts['Concepts']\n",
    "comparison_df['LLM_Concepts'] = comparison_df['Question'].apply(\n",
    "    lambda q: '; '.join(simulated_llm.extract_concepts(q))\n",
    ")\n",
    "\n",
    "print(\"Comparison of Extraction Methods:\")\n",
    "for idx, row in comparison_df.iterrows():\n",
    "    print(f\"\\n=== Question {row['Question Number']} ===\")\n",
    "    print(f\"Question: {row['Question'][:150]}...\")\n",
    "    print(f\"Hybrid: {row['Hybrid_Concepts']}\")\n",
    "    print(f\"LLM: {row['LLM_Concepts']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d78120a1",
   "metadata": {},
   "source": [
    "## 7. Test Multiple Subjects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af5f2793",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with different subjects\n",
    "subjects = ['ancient_history', 'economics', 'mathematics', 'physics']\n",
    "\n",
    "for subject in subjects:\n",
    "    try:\n",
    "        questions_file = f\"resources/{subject}.csv\"\n",
    "        custom_dict_file = f\"dictionaries/{subject}_concepts.csv\"\n",
    "        \n",
    "        if os.path.exists(questions_file):\n",
    "            df = read_questions_csv(questions_file)\n",
    "            if not df.empty:\n",
    "                extractor = ConceptExtractor(custom_dict_file=custom_dict_file)\n",
    "                result_df = extractor.extract_concepts_from_dataframe(df.copy())\n",
    "                \n",
    "                print(f\"\\n=== {subject.upper()} ===\")\n",
    "                print(f\"Questions processed: {len(result_df)}\")\n",
    "                \n",
    "                # Show first question as example\n",
    "                if len(result_df) > 0:\n",
    "                    first_row = result_df.iloc[0]\n",
    "                    print(f\"Sample Question: {first_row['Question'][:100]}...\")\n",
    "                    print(f\"Sample Concepts: {first_row['Concepts']}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {subject}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb60f9ef",
   "metadata": {},
   "source": [
    "## 8. Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c9c9863",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the results to CSV\n",
    "output_file = \"notebook_output_concepts.csv\"\n",
    "output_df = questions_with_concepts[['Question Number', 'Question', 'Concepts']]\n",
    "output_df.to_csv(output_file, index=False, encoding='utf-8')\n",
    "\n",
    "print(f\"Results saved to {output_file}\")\n",
    "print(f\"\\nSummary:\")\n",
    "print(f\"Total questions processed: {len(output_df)}\")\n",
    "questions_with_concepts_count = len(output_df[output_df['Concepts'].str.len() > 0])\n",
    "print(f\"Questions with extracted concepts: {questions_with_concepts_count}\")\n",
    "print(f\"Coverage: {questions_with_concepts_count/len(output_df)*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c7da387",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This project demonstrates a hybrid approach to concept extraction that:\n",
    "\n",
    "1. **Combines RAKE with custom dictionaries** for accurate, domain-specific extraction\n",
    "2. **Avoids expensive LLM API calls** while maintaining good performance\n",
    "3. **Provides a framework for future LLM integration** when needed\n",
    "4. **Works across multiple domains** (History, Economics, Mathematics, Physics)\n",
    "5. **Offers both programmatic and interactive interfaces**\n",
    "\n",
    "The system is cost-effective, adaptable, and ready for production use in educational and assessment applications."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
